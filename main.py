import ollama
from document_extraction.read_pdf_as_plain import read_pdf_as_plain
from src.timing_decorator.timer import measure_time


client = ollama.Client()

@measure_time
def generate_response(client, model, prompt):
    """
    Generates a response from a given model using the provided client.

    Parameters:
    -----------
    client : object
        The client instance used to interact with the model.
        It must have a `generate` method that accepts `model` and `prompt`.

    model : str
        The name or identifier of the model to use for generating the response.

    prompt : str
        The input text or prompt that the model will respond to.

    Returns:
    --------
    response : object
        The output generated by the model. The exact format depends on the `client.generate` implementation.
    """
    return client.generate(model=model, prompt=prompt)

model = "qwen_multiple_sentence_to_graph"

file_path = "inputs/Multiple_sentences.pdf"
output_dir = "data/extracted_jsons"

file = read_pdf_as_plain(file_path, output_dir)

first_page_content = file['pages'][0]['content']

questions = f"""
INPUT_TEXT:
{first_page_content}

TASK:
Parse INPUT_TEXT and produce a single JSON object following the OUTPUT SCHEMA defined in the SYSTEM message. Parse all sentences (1â€“100), deduplicate nodes across sentences when clearly the same entity, create relationships and parameterized Cypher to MERGE nodes and relationships, provide cypher_params, list all clues with sentence_index and spans, run cross-sentence validation checks, and reconstruct each input sentence from the graph. Do not output anything except the single JSON object.
"""

# prompt = f"{first_page_content} \n {questions}"
prompt = questions

response = generate_response(client, model, prompt)

print(f"Response from Ollama \n{response.response}")
