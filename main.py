import ollama
from document_extraction.read_pdf_as_plain import read_pdf_as_plain
from src.timing_decorator.timer import measure_time


client = ollama.Client()

@measure_time
def generate_response(client, model, prompt):
    """
    Generates a response from a given model using the provided client.

    Parameters:
    -----------
    client : object
        The client instance used to interact with the model.
        It must have a `generate` method that accepts `model` and `prompt`.

    model : str
        The name or identifier of the model to use for generating the response.

    prompt : str
        The input text or prompt that the model will respond to.

    Returns:
    --------
    response : object
        The output generated by the model. The exact format depends on the `client.generate` implementation.
    """
    return client.generate(model=model, prompt=prompt)

model = "llama3.2-3B-graph"

file_path = "inputs/Mult_sentences.pdf"
output_dir = "data/extracted_jsons"

file = read_pdf_as_plain(file_path, output_dir)

first_page_content = file['pages'][0]['content']

questions = f'''
Sentences:
{first_page_content}

Instructions for the model (must be followed):
- Produce the five outputs in order (NODES_JSON, RELATIONS_JSON).
- Generate a CYPHER creation of the nodes and relations provided above.
- Use the rules in the SYSTEM message for labels, ids, date parsing, and Cypher generation.
- Keep everything minimal and machine-parseable.
- Output now.
'''

# prompt = f"{first_page_content} \n {questions}"
prompt = questions

response = generate_response(client, model, prompt)

print(f"Response from Ollama \n{response.response}")
